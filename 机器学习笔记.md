# 聚类算法

## 基础内容

* 什么是好的聚类结果：”簇内相似度“(intra-cluster similarity)低，”簇间相似度“(inter-cluster similarity)高；
* 聚类性能结果的度量  
  * 外部指标：将聚类结果与某个参考模型比较
  * 内部指标：直接考察聚类结果而不利用任何参考模型
* 对**离散变量**的处理
  * 有序属性(如小、中、大)可以直接当做连续变量处理
  * 无序属性：
    * 采用VDM(Value Difference Metric)处理；
      * 问题：VDM在计算时需要样本所在簇的信息，在聚类时如何获取该信息？
      * 回答：当聚类算法以距离为基础进行计算时，会给样本初始化一个簇的类别，因此不会影响计算(典型例子K-Means算法)；
    * 采用one-hot编码处理，但可能导致信息损失。
* 用于相似度度量的距离未必满足距离度量的所有性质，尤其是直递性(在某些场景内，如西瓜书中人、马、人马的例子)

## 典型聚类算法

### 1. K-Means算法

算法本质：最小化平方误差(簇内差异最小化)
$E=\sum_{i=1}^{k}\sum_{x\in{C_i}}(x-\mu_i)^2$，其中$k$代表簇的个数，$C_i$代表第i个簇的样本集合，$\mu_i$代表第$i$个簇的均值。设计贪心算法求解该优化问题。

算法输入：样本集合$\{x_1,x_2,...,x_m\}$，簇的个数$k$

算法逻辑：


1. 随机初始化$k$个样本作为初始均值向量；
2. 根据距离计算结果，将所有样本分配到$k$个簇中；
3. 更新$k$个簇中的均值向量；
4. 检查各簇的均值向量是否有更新。如果无变化，则算法结束；如果有变化，则跳转到第2步继续循环。

为了避免算法运行时间过长，一般会限制迭代次数或调整幅度过小时停止算法。

算法缺陷：K-Means算法对**离群点**非常敏感。因为算法要求将每个点都分配到一个簇内，并且优化目标是簇内差异最小化。

K-Medoids算法：是对K-Means算法的改进，区别仅在于K-Means以簇内的均值向量为聚类中心，K-Medoids算法以簇内实际存在的样本为聚类中心；K-Medoids算法计算效率相对较低。

### 2. 学习向量量化(LVQ)

LVQ(Learning Vector Quantizetion)是一种比较特殊的聚类算法。该算法要求输入的样本集中包括类别标记。

算法输入：样本集合$\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\}$，簇的个数$q$，学习率$\eta\in(0,1)$

算法本质：

* 每个原型向量$p_i$定义了与之相关的一个区域$R_i$，该区域中每个样本与$p_i$的距离不大于它与其他原型向量$p_{i'}$的距离，由此形成了对样本空间$\chi$的簇划分$\{R_1,R_2,...,R_q\}$，该划分通常称为“Voronoi剖分”。
* 学习一组原型向量$\{p_1,p_2,...,p_q\}$描述各个簇的特征。这点和K-Means算法有相似之处，只不过K-Means算法中的原型向量是簇内的均值向量。

算法逻辑:

1. 初始化一组原型向量$\{p_1,p_2,...,p_q\}$；
2. 随机选取一个样本$(x_j,y_j)$，找到与$x_j$距离最近的原型向量$p_{i*}$；
3. 如果$y_j=t_{i*}$，那么$p'=p_{i*}+\eta\cdot(x_j-p_{i*})$；否则$p'=p_{i*}-\eta\cdot(x_j-p_{i*})$；
4. 将原型向量$p_{i*}$更新为$p'$；如果达到终止条件(如达到最大迭代次数，原型向量更新很小)则算法结束。

可能的用处：

* 将原始的样本集划分为**更多**的簇或**更少**的簇；
* 用原型向量描述一个簇的特征，可以与其他算法结合起到提高效率的作用。例如在KNN算法中，将遍历特定簇内所有样本这一步改为和该簇对应的**原型向量**进行比较。

一些问题：

* 为什么LVQ算法第2步中要随机选取一个样本，而不是在样本集合中进行遍历？

### 3. 高斯混合聚类
